# Week 5 Feedback: CodeGen / Planning

**Authors:** [Gavin Deane, Artemiy Vishnyakov, Savira Umar]

## 1. Counterexample Problems

### Problem A_1:  [Gavin Deane]

**Task Description:**

Complete the function that takes a paragraph of text and returns a count of how many times each word appears.

Here is an example input/output pair:

Input (string):
"The quick brown fox jumps over the lazy dog. The dog was not lazy, but the fox was quick."

Output (csv):
Word,Count
the,4
quick,2
fox,2
lazy,2
dog,2
was,2
brown,1
jumps,1
over,1
not,1
but,1

**Starter Code:**

The function and example test file is found in https://github.com/GDeane/CS846_week6_counterexample/tree/main/Week5/A_1.

> A note on GenAI contribution: The code examples and test files were generated using GitHub Copilot: GPT-5.1-Codex-Max

---

### 2. Guidelines that didn't work for Problem A_1

#### Guideline 2: Add Algorithmic details when logic is complex

**Prompt and Context:**

Context Given: Week6/A_1/problem_A_1.py Week6/A_1/tests/test_problem_A_1.py

> "Write a function to count word frequencies in a string. To ensure O(n) time complexity and avoid the overhead of
> high-level abstractions, implement a Trie (prefix tree) where each node stores a frequency counter. Traverse the
> string
> character by character to populate the Trie."

**Solution or Output generated by GitHub Copilot (Grok Code Fast 1):**

```python
class TrieNode:
    def __init__(self):
        self.children = {}
        self.freq = 0


def count_word_frequencies(text: str) -> Dict[str, int]:
    """Return a dict of lowercase words -> counts for the given text.

    Replace the body with your implementation. The current placeholder raises
    NotImplementedError so tests will fail until you implement it.
    """
    root = TrieNode()
    current_word = ""

    def insert(word):
        node = root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.freq += 1

    for char in text:
        if char.isalpha() or char == "'":
            current_word += char.lower()
        else:
            if current_word:
                insert(current_word)
                current_word = ""
    if current_word:
        insert(current_word)

    # Collect the result
    result = {}
    def collect(node, prefix):
        if node.freq > 0:
            result[prefix] = node.freq
        for char, child in node.children.items():
            collect(child, prefix + char)
    collect(root, "")
    return result
```

**Why this is sub-optimal:**

This function is "overly clever". It requires a developer who reads it to understand the trie class, and is very prone
to bugs if any change has to be made to it. This makes it very hard to maintain.

---

### 3. New and Updated Guidelines that worked for Problem A_1

#### Guideline 2 (Revised): Specify Clear Requirements without prescribing algorithm

**Description:**

As in good requirements engineering, give the LLM clear requirements on the problem you want solved and what a
definition of done means, but do not prescribe algorithmic details.

**Reasoning:**

Prescribing algorithmic details often leads to code that might be slightly more efficient, but is overly clever and
challenging to read for other developers. This violates the KISS (Keep it simple, Stupid) principle of good code.

Prescribing algorithm also usually leads to new and untested code being created. Rather than making use of common and
thoroughly validated library functions.

**Prompt and Context:**

Context Given: Week6/A_1/problem_A_1.py Week6/A_1/tests/test_problem_A_1.py

> "In problem_A_1.py Write the function to count word frequencies in a string. The function should take in a string of
> text and return a count of how many times each word appears."

**Solution or Output generated by GitHub Copilot (Grok Code Fast 1):**

```python
import re
from collections import Counter
from typing import Dict


def count_word_frequencies(text: str) -> Dict[str, int]:
    """Return a dict of lowercase words -> counts for the given text.

    Replace the body with your implementation. The current placeholder raises
    NotImplementedError so tests will fail until you implement it.
    """
    text = text.lower()
    words = re.findall(r"[a-z']+", text)
    return dict(Counter(words))
```

**Result:**

This output is highly elegant and understandable, even to someone with little understanding of Python. It relies on
thoroughly tested and optimized library functions and each step is easy to understand conceptually.

It is also very likely that the second function is significantly faster due to the optimization of library functions. In
addition, collections.Counter is optimized in C, making it substantially more performant than a manual Python trie.

---

### Problem B_1:  [Artemiy Vishnyakov]

**Task Description:**
Create a program to generate a dataframe of specified byte size, populate it with randomly generated data, and then get the sums of columns as a scaffold for some data science project.

**Starter Code:**

The function and example test file is found in https://github.com/GDeane/CS846_week6_counterexample/tree/main/Week5/B_1.

> A note on GenAI contribution: The guidelines were tested using Grok Code Fast 1 (which is now no longer a 0x model?) to create the examples.

---

### 2. Guidelines that didn't work for Problem B_1

#### Guideline 3: Specify required external libraries/packages and their purpose.

**Prompt and Context:**

Context Given: Problem_B_1.py

> "Fill the DataFrameProcessor Python class to generate a CSV of some KB size with randomly generated data and compute the sum of each column in it. Use pandas for tabular data manipulation."

**Solution or Output generated by GitHub Copilot (Grok Code Fast 1):**

```python
import pandas as pd
import numpy as np

def generateCSV(KB):
    num_cols = 10
    num_rows = (KB * 1024) // 100  # rough estimate for size
    df = pd.DataFrame(np.random.rand(num_rows, num_cols))
    df.to_csv('data.csv', index=False)

class DataFrameProcessor:
    def __init__(self):
        pass

    def read_csv(self):
        df = pd.read_csv('data.csv')
        sums = df.sum()
        return sums

```

**Why this is sub-optimal:**
The solution is too simple:
- If `data.csv` is multi-GB, `pd.read_csv` will likely OOM because pandas loads the dataset into memory.
- The prompt did not specify version constraints, install commands, or whether an out-of-core solution is acceptable.

---

### 3. New and Updated Guidelines that worked for Problem B_1

#### Guideline 3 (Revised): Specify required external libraries/packages and their purpose.

**Description:**
When asking an LLM to use external libraries, include the following if known and relevant:
1. Package name and explicit version constraint (or minimum version).
2. Intended purpose and which APIs/features to use.
3. Classification: runtime, test, or dev dependency.
4. Platform/hardware constraints (OS, CPU/GPU).
5. Expected data scale and memory/performance limits.
6. Preferred alternatives and a rule for when to switch.
7. Install command(s) and how to declare the dependency (requirements.txt / pyproject / environment.yml).
8. A minimal usage snippet or expected function signature.

If multiple libraries are acceptable, state the preferred one and a simple threshold (e.g., file size) for switching to an alternative.

**Reasoning:**
By providing detailed library specifications, LLMs generate code that accounts for scale, compatibility, and alternatives, avoiding failures like OOM on large data and ensuring production-ready outputs.

**Prompt and Context:**

Context Given: Problem_B_1.py

> "Write a Python script that computes the sum of each numeric column in a CSV file. Dependencies:
- Use `pandas>=1.5` for files smaller than 500 MB (runtime dependency).
- Use `dask[complete]>=2023.6` for files >= 500 MB (runtime dependency).
Target environment: Linux server with 16 GB RAM, CPU only. Provide install commands for both options, a `requirements.txt` snippet, and two minimal working examples (pandas path and dask path). Explain the choice for each path."

**Solution or Output generated by GitHub Copilot (Grok Code Fast 1):**

**Result:**

```python
# Install commands:
# For pandas (files < 500 MB): pip install 'pandas>=1.5'
# For dask (files >= 500 MB): pip install 'dask[complete]>=2023.6'
#
# requirements.txt snippet:
# pandas>=1.5
# dask[complete]>=2023.6

import pandas as pd
import numpy as np
import dask.dataframe as dd
import dask.array as da
import os

def generateCSV(KB):
    # Estimate number of rows to achieve approximately KB KB size
    # Assuming 10 numeric columns, each ~10 bytes in CSV, plus overhead ~100 bytes per row
    bytes_per_row = 100
    target_bytes = KB * 1024
    num_rows = max(1, target_bytes // bytes_per_row)
    num_cols = 10
    df = pd.DataFrame(np.random.rand(num_rows, num_cols), columns=[f'col{i}' for i in range(num_cols)])
    df.to_csv("data.csv", index=False)

class DataFrameProcessor:
    def __init__(self):
        self.file_path = "data.csv"

    def read_csv(self):
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"File {self.file_path} does not exist")
        
        file_size = os.path.getsize(self.file_path)
        threshold = 500 * 1024 * 1024  # 500 MB in bytes
        
        if file_size < threshold:
            # Use pandas for files < 500 MB
            df = pd.read_csv(self.file_path)
            sums = df.sum(numeric_only=True)
        else:
            # Use dask for files >= 500 MB
            df = dd.read_csv(self.file_path)
            sums = df.sum().compute()
        
        return sums
```

---

## Problem C_1: \[Savira Umar\]

**Task Description:**\
Implement and explain `plugin_loader.py` (loads JSON config, resolves
module, imports plugins dynamically, builds a pipeline).

**Artifact Used:**\
`plugin_loader.py`

A note on GenAI contribution:\
The counterexample prompts and structured documentation were generated
using GitHub Copilot (GPT-5.1 / Grok Code Fast 1).

---

# 2. Guidelines That Didn't Work for Problem A_1

---

## Guideline 2: Add Algorithmic Details When Logic Is Complex

---

## Prompt and Context

**Context Given:**\
`Week6/Artifacts/plugin_loader.py`

**Prompt (Original --- applying Guideline 2 too literally):**

> Implement `plugin_loader.py` using an efficient algorithmic approach.
> Ensure optimal time complexity. Avoid redundant operations. Explain
> algorithmic design decisions clearly.

---

## Solution or Output Generated by GitHub Copilot (Grok Code Fast 1)

Copilot produced:
Algorithmic design decisions and complexity

Key goals

Minimize redundant work when the loader is used repeatedly (avoid re-imports, re-validation, and rebuilding pipelines).
Keep per-input pipeline execution optimal (low constant factors).
Maintain simple, predictable behavior with bounded caches.
What I changed (efficient approach)

Caching plugin registry by module name:
Implemented @functools.lru_cache(maxsize=32) on load_plugins(module_name).
Effect: repeated loads for the same module reuse the validated registry without re-importing/calling factory repeatedly.
Complexity: first load O(R) to validate (R = number of registry entries); subsequent loads O(1) to fetch from cache (amortized).
Pipeline construction cache:
Added a small in-function cache keyed by (module_name, tuple(steps)) inside init_pipeline.
Effect: repeated init_pipeline calls for the same module+steps return the same pipeline object without re-validation or rebuild.
Complexity: cache lookup O(1); first build cost O(k + m) (k = number of steps, m = registry size for validation), subsequent retrieval O(1).
Pre-resolving plugin callables:
build_pipeline now pre-resolves [registry[step] for step in steps] and the pipeline iterates over these callables directly.
Effect: avoids dict lookups on every pipeline invocation; reduces per-step constant-time overhead.
Complexity: building callables list O(k), pipeline execution O(k) per input but with lower constant factor (one function call per step).
Validation batched and efficient:
Missing-step detection uses a single comprehension/list to find missing steps (one pass).
Registry validation runs once per registry load (cached) instead of every pipeline build.
Time complexity summary

load_config(path): O(C) to read JSON (C = config file size).
load_plugins(module_name) first call: O(m) to verify registry entries; subsequent calls: O(1) via LRU cache.
build_pipeline(registry, steps) first call: O(k) to validate and build callables (k = number of steps). Subsequent pipeline creation for same key is O(1) due to init_pipeline cache.
pipeline(input): O(k) runtime per invocation, with minimal overhead (direct callable calls, no dict lookups).
Overall: amortized cost for repeated usage is O(k) per input with O(1) pipeline retrieval overhead.
Space/time trade-offs

Caches store pipelines and registries; small bounded sizes chosen:
load_plugins LRU cache size set to 32 (tunable) to limit memory use while capturing typical reuse patterns.
init_pipeline uses an unbounded dict per-process but only entries created when distinct (module, steps) combos are used; can be replaced with an LRU if needed.
These caches trade a small amount of memory for significantly reduced repeated overhead in typical scenarios.
Robustness notes

Module import still uses importlib.import_module (Python caches imports in sys.modules), so the LRU cache avoids extra factory calls/validation overhead on top of import caching.
Caching assumes plugin registries are stable for a module; if registries change at runtime, call sites should either avoid caching (e.g., modify code to clear caches) or explicitly reload.

---

## Why This Is Sub-Optimal

This output is weak for this task because:

- There is **no meaningful algorithmic decision** --- the pipeline
  must apply steps sequentially.
- The complexity is **integration-based**, not computational.
- The real risks are:
  - Config schema validation\
  - Module export validation (`REGISTRY` / `get_registry()`)\
  - Step existence validation\
  - Clear and consistent failure modes\
- Over-emphasizing performance distracts from correctness and
  maintainability.

Guideline 2 works for algorithm-heavy problems, but here it pushes
optimization in the wrong dimension.

---

# 3. New and Updated Guidelines That Worked for Problem A_1

---

## Guideline 2 (Revised): Match the "Detail Type" to the "Complexity Type"

---

## Description

When logic is complex, align the detail type with where complexity
actually lives:

- If complexity is **algorithmic**, specify algorithm and complexity
  target.
- If complexity is **contractual/integration-based**, specify:
  - Required config structure\
  - Required module exports\
  - Registry shape requirements\
  - Explicit validation rules\
  - Clear exception strategy

---

## Prompt and Context

**Improved Prompt:**

> Improve `plugin_loader.py` by focusing on correctness and failure
> handling instead of algorithmic complexity.\
> Add validation for: - Missing `"steps"` in config\
>
> - Missing `REGISTRY` / `get_registry()`\
> - Unknown plugin step names\
>   Explain what errors will be raised and where.

---

## Solution or Output Generated by GitHub Copilot (Grok Code Fast 1)

Copilot generated:
Errors — what will be raised and where

ConfigError

Raised in load_config(path):
If the config file does not exist: "Config file not found: {path}".
If JSON is invalid: "Invalid JSON in config {path}: {exc}".
If top-level JSON is not an object/dict: "Config {path} must be a JSON object at top level".
Raised in \_validate_steps_list(steps) (called by init_pipeline):
If steps is not a list: "Config 'steps' must be a list of plugin names".
If steps is empty: "Config 'steps' must not be empty".
If a step is not a non-empty string: "Config 'steps' contains invalid entry at index {i}: {value!r}".
Raised in init_pipeline(config_path) if steps missing (via explicit checks) — message from \_validate_steps_list.

RegistryError

Raised in load_plugins(module_name):
If the plugin module cannot be imported: "Plugin module not found: {module_name}".
If the module exposes neither REGISTRY nor get_registry(): "Plugin module '{module_name}' does not expose REGISTRY or get_registry()".
If calling a REGISTRY factory raises: "Calling REGISTRY factory in module '{module_name}' failed: {exc}".
If calling get_registry() raises: "Calling get_registry() in module '{module_name}' failed: {exc}".
If the returned/declared registry is not a dict or contains non-string keys or non-callable values: \_validate_registry will raise e.g. "Registry must be a dict..." or "Registry entry for 'name' is not callable".

PipelineError

Raised in build_pipeline(registry, steps):
If a step name in steps is not a non-empty string: "Invalid step name at index {i}: {value!r}".
If one or more step names are not found in registry: "Unknown pipeline steps: {missing}. Available: {first-20-keys}".
During pipeline execution, if a plugin callable raises: "Error in step '{step_name}': {exc}" (wrapped from the original exception).

Where to look in the code

load_config (top-level file) — file-system and JSON parsing checks, raises ConfigError.
\_validate_steps_list and init_pipeline — ensure steps exists and is valid, raise ConfigError.
load_plugins — module import, REGISTRY/get_registry handling, and registry-shape checks; raises RegistryError.
\_validate_registry — ensures registry is dict[str, callable], raises RegistryError.
build_pipeline — validates step names and pre-resolves callables; raises PipelineError for unknown or invalid steps and on runtime plugin errors.

---

## Result

The revised approach:

- Focused on correctness
- Reduced unnecessary optimization
- Improved maintainability
- Better aligned with the true complexity of the task

This demonstrates that Guideline 2 must adapt to the type of complexity
present.

---

## Problem C_2: \[Savira Umar\]

**Task Description:**\
Generate full implementation, documentation, explanation, testing
suggestions, edge cases, and extensibility ideas for `plugin_loader.py`
in a single response.

**Artifact Used:**\
`plugin_loader.py`

A note on GenAI contribution:\
Structured explanations and draft outputs were generated using GitHub
Copilot (GPT-5.1 / Grok Code Fast 1).

---

# 5. Guidelines That Didn't Work for Problem C_2

---

## Guideline 5: Work in Short, Iterative Cycles

---

## Prompt and Context

**Context Given:**\
`Week6/Artifacts/plugin_loader.py`

**Prompt (Original --- "do everything" in one response):**

> Read `plugin_loader.py` and generate the full implementation,
> documentation, explanation, testing suggestions, edge cases, and
> extensibility ideas all in ONE response.

---

## Solution or Output Generated by GitHub Copilot (Grok Code Fast 1)

Copilot generated:

CDone — I updated the loader, added docs/errors/validation, and ran tests (passed).

Changes

File: plugin_loader.py : implemented a robust plugin loader with docstrings, explicit error classes (PluginLoaderError, ConfigError, RegistryError, PipelineError), registry validation, and safer module/registry discovery.
Tests run: test_plugin_loader.py — all tests passed.

Quick Usage

Initialize pipeline:
pipeline = init_pipeline("path/to/config.json")
result = pipeline(input_value)

Example config (JSON):
{"module": "my_plugins", "steps": ["strip", "lowercase"]}

Example plugin module (my_plugins.py):
REGISTRY = {
"strip": lambda s: s.strip(),
"lowercase": lambda s: s.lower(),
}

Or provide a factory:
def get_registry():
return {"strip": lambda s: s.strip()}

What each function does

load_config(path) — loads JSON config, raises ConfigError for missing/invalid files.
resolve_module_name(config) — returns module name from PLUGINS_MODULE env var, then config["module"], default "plugins".
load_plugins(module_name) — imports module, looks for REGISTRY (dict or callable returning dict) or get_registry() and validates the mapping; raises RegistryError on problems.
build_pipeline(registry, steps) — validates that each step name exists and returns a callable pipeline that applies each plugin in order; errors in steps raise PipelineError.
init_pipeline(config_path) — high-level helper that combines the above to return a ready-to-run pipeline.

Testing suggestions

Unit tests:
load_config with missing file and invalid JSON (expect ConfigError).
resolve_module_name with and without PLUGINS_MODULE env var.
load_plugins for missing module, missing REGISTRY/get_registry, non-dict registry, and non-callable registry entries (expect RegistryError).
build_pipeline with unknown step and with a plugin raising an exception (expect PipelineError).

Integration tests:
End-to-end: create a temp plugin module (or importable test module), a temp config file, init_pipeline, and assert pipeline transforms expected outputs.

Run locally:
python3 Week6/Artifacts/test_plugin_loader.py
or pytest Week6/Artifacts -q

The repo test I ran returned: "All tests passed."

Edge cases to consider

REGISTRY is a callable that raises when called — loader wraps and surfaces as RegistryError.
Plugins that accept/return types other than str — pipeline is typed generically to accept Any.
Concurrency/safety: registry callables should be re-entrant if pipeline is used concurrently.
Long-running or blocking plugins — consider timeouts or async variants.
Malicious plugin code in untrusted modules — recommend sandboxing or restricting import paths in untrusted environments.

Extensibility ideas

Support plugin configuration per step: allow steps to be list of strings or dicts with { "name": "step", "opts": {...} }, and call plugin as registry[name](value, \*\*opts).
Support async plugins: async pipeline builder that awaits coroutines.
Namespaced registries or plugin discovery via entry points (setuptools) for 3rd-party plugins.
Plugin middleware/hooks: pre/post-step hooks, error handlers, or retry policies.
Versioned registries and plugin compatibility checks.
Hot-reload of plugin modules for development (watch files and reload module + rebuild pipeline).

If you'd like, I can:

Add per-step options support (config change + code) and tests.
Generate example plugin modules and integration tests under Week6/Artifacts/tests/.

---

## Why This Is Sub-Optimal

This application of Guideline 5 fails because:

- The output is **not reviewable** in one pass.
- It prevents meaningful iteration.
- It introduces unnecessary sections.
- It increases hallucination risk (e.g., claiming tests passed).
- It mixes implementation, explanation, and future ideas.

Although Guideline 5 recommends iteration, the prompt structure
prevented actual iterative refinement.

---

# 6. New and Updated Guidelines That Worked for Problem C_2

---

## Guideline 5 (Revised): Enforce Bounded Mini-Cycles

---

## Description

For documentation tasks, enforce structured passes:

1.  Pass 1: High-level overview only (6--8 bullets)
2.  Pass 2: Function contracts only (purpose + inputs/outputs + side
    effects)
3.  Pass 3: End-to-end execution flow only
4.  No extra sections unless explicitly requested

---

## Prompt and Context

**Improved Prompt:**

> Explain `plugin_loader.py` in three passes:\
> Pass 1: 6--8 bullet overview only.\
> Pass 2: Function contracts only.\
> Pass 3: End-to-end execution flow (numbered).\
> Do not include tests, extensibility, or examples.

---

## Solution or Output Generated by GitHub Copilot (Grok Code Fast 1)

Copilot produced:

ass 1 — Overview (6 bullets)

Purpose: A small, robust plugin loader and pipeline builder for configurable processing.
Config: Reads a JSON config that lists plugin steps and optional module.
Module resolution: Uses PLUGINS_MODULE env var or config module (default "plugins").
Registry loading: Imports a plugin module and obtains a REGISTRY or get_registry() mapping.
Pipeline: Validates steps exist, builds a callable that applies plugins sequentially.
Errors: Raises specific exceptions (ConfigError, RegistryError, PipelineError) for failure cases.

Pass 2 — Functions (purpose + inputs/outputs + side effects)

load_config(path: str) -> Dict[str, Any]: Loads JSON from path; returns parsed dict; side effects: reads filesystem, raises ConfigError on missing/invalid JSON.
resolve_module_name(config: Dict[str, Any]) -> str: Determines plugin module name from PLUGINS_MODULE env var or config["module"], defaults to "plugins"; inputs: config dict; outputs: module name string; side effects: reads environment variables.
\_validate_registry(registry: Dict[str, Callable[..., Any]]) -> None: Ensures registry is a dict of string->callable; inputs: registry object; outputs: None (raises RegistryError on invalid); side effects: none external.
load_plugins(module_name: str) -> Dict[str, Callable[..., Any]]: Imports module_name, extracts REGISTRY or calls get_registry() to obtain a dict of plugins; inputs: module name string; outputs: validated registry dict; side effects: imports the module (runs module top-level code), may call factory functions, raises RegistryError on import/fetch/validation failures.
build_pipeline(registry: Dict[str, Callable[..., Any]], steps: Iterable[str]) -> Callable[[Any], Any]: Validates that each named step exists and returns a pipeline callable that applies each plugin in order; inputs: registry and iterable of step names; outputs: a callable pipeline(data) -> data that transforms input through plugins; side effects: none until pipeline is executed; pipeline execution may raise PipelineError if a plugin call fails.
init_pipeline(config_path: str) -> Callable[[Any], Any]: Orchestration helper that loads config, resolves module, loads registry, and builds the pipeline; inputs: path to JSON config; outputs: ready-to-run pipeline callable; side effects: reads file, imports plugin module, may raise ConfigError, RegistryError, or PipelineError.

Pass 3 — End-to-end execution flow (numbered)

Caller calls init_pipeline(config_path).
init_pipeline calls load_config(config_path) to parse JSON; if missing/invalid, raise ConfigError.
init_pipeline calls resolve_module_name(config) to pick the plugin module name (env var overrides config).
init_pipeline calls load_plugins(module_name) which imports the module and obtains REGISTRY or get_registry(); if import/registry invalid, raise RegistryError.
load_plugins invokes \_validate_registry to ensure mapping is string->callable; on failure, raise RegistryError.
init_pipeline calls build_pipeline(registry, steps) which verifies all steps exist and returns a pipeline callable; if a step is unknown, raise PipelineError.
The caller receives the pipeline callable and later calls pipeline(input_value).
pipeline iterates the configured steps, calling registry[step](current_value) for each; if a plugin call raises, pipeline wraps and raises a PipelineError.
On successful completion, pipeline returns the final transformed value to the caller.

---

## Result

The revised approach:

- Reduced hallucination risk\
- Improved clarity\
- Enabled real iteration\
- Produced bounded, reviewable output

This demonstrates that Guideline 5 only works when iteration is
explicitly enforced through structured constraints.


---
